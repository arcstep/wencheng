{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载 .env 到环境变量\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_documents_folder = os.getenv(\"LANGCHAIN_CHINESE_DOCUMENTS_FOLDER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain_openai import OpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!poetry add lancedb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_extension(filename: str) -> str:\n",
    "    return filename.split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileLoadFactory:\n",
    "    @staticmethod\n",
    "    def get_loader(filename: str):\n",
    "        filename = filename.strip()\n",
    "        ext = get_file_extension(filename)\n",
    "        if ext == \"pdf\":\n",
    "            return PyPDFLoader(filename)\n",
    "        elif ext == \"docx\":\n",
    "            return Docx2txtLoader(filename)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"File extension {ext} not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/xuehongwei/github/wencheng/data/documents/gd_tax_arch.docx']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "_documents_folder = os.getenv(\"LANGCHAIN_CHINESE_DOCUMENTS_FOLDER\")\n",
    "# current_working_directory = os.getcwd()\n",
    "# print(\"当前工作目录的绝对路径:\", current_working_directory)\n",
    "\n",
    "def get_files(directory, extensions):\n",
    "  files = []\n",
    "  for dirpath, dirnames, filenames in os.walk(directory):\n",
    "    for filename in filenames:\n",
    "      if os.path.splitext(filename)[1] in extensions:\n",
    "        files.append(os.path.join(dirpath, filename))\n",
    "  return files\n",
    "\n",
    "# 使用示例\n",
    "get_files(_documents_folder, ['.docx', '.pdf'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs(filename: str) -> List[Document]:\n",
    "    file_loader = FileLoadFactory.get_loader(filename)\n",
    "    pages = file_loader.load_and_split()\n",
    "    return pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(_documents_folder, ['.docx', '.pdf'])\n",
    "\n",
    "raw_docs = load_docs(files[0])\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000,\n",
    "                        chunk_overlap=200,\n",
    "                        length_function=len,\n",
    "                        add_start_index=True,\n",
    "                    )\n",
    "documents = text_splitter.split_documents(raw_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在长期的业务应用建设过程中，没有统一的技术规范要求，建设的应用系统技术多样，差异化明显，系统应用升级方式多样，难以进行统一标准化的管理，系统应用升级方法和工具参差不齐，亟待规范统一系统开发、测试、部署等技术。\n",
      "\n",
      "（四）数据中心级的容灾能力亟待改善\n",
      "\n",
      "机构改革后，以税务总局的数据灾备要求文件为指导，合并的数据中心未形成整体容灾机制。从容灾能力要素分析，目前缺乏跨数据中心层级的备用数据处理系统、备用网络系统、备用基础设施、专业技术支持能力、运行维护管理能力以及灾难恢复预案；从业务应用需求分析，未对业务系统进行容灾需求评估分析，缺乏业务系统容灾指标（RPO恢复点目标/RTO恢复时间目标）。\n",
      "\n",
      "当前以南海、五山为主要的生产数据中心，海珠、江门为备份数据中心。南海、五山各自数据中心内主机故障恢复能力不完善，当整个数据中心发生火灾、网络瘫痪等灾难事件时，没有数据中心能够提供灾难恢复能力，无法保障业务连续运转。灾难恢复能力是保障业务连续的基石，亟需建立灾难恢复策略和完善灾难恢复能力。\n",
      "\n",
      "组织管理\n",
      "\n",
      "近年来，广东省税务局的信息化建设取得骄人的成绩，但在信息化建设自主可控能力、信息化人才管理、信息化制度、信息化创新能力四个方面仍有持续改进与提升的空间。主要问题总结如下：\n",
      "\n",
      "（一）信息化建设自主可控能力有待提升\n",
      "\n",
      "信息化建设大多采用项目制且外部购买服务的建设策略，内部以抽调人员临时组建项目攻关团队的形式进行配合与承接，整体上存在项目管理被动、系统迭代不及时等弊端，在信息化项目的开发、运维、管控上缺少一定的主导权。\n",
      "\n",
      "（二）信息化人才有待扩充\n",
      "\n",
      "广东省税务局现有的信息化人才主要分为两类：一是从信息化部门组成来看，人员构成包括征管科技处、税收大数据和风险管理局、信息中心人员和部分由市（区）税务局调派的人员；二是从信息化项目规划、需求、设计、实施和运维来看，规划、需求和设计阶段以广东省税务局内部人员为主，开发实现与运维阶段以第三方供应商为主。\n"
     ]
    }
   ],
   "source": [
    "print(documents[22].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_document(filename: str, lanceFile: str):\n",
    "  \"\"\"将文档内容向量化，并存放LanceDB\"\"\"\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_docment(\n",
    "        filename: str,\n",
    "        query: str,\n",
    ") -> str:\n",
    "    \"\"\"根据一个PDF文档的内容，回答一个问题\"\"\"\n",
    "\n",
    "    raw_docs = load_docs(filename)\n",
    "    if len(raw_docs) == 0:\n",
    "        return \"抱歉，文档内容为空\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "                        chunk_size=1000,\n",
    "                        chunk_overlap=200,\n",
    "                        length_function=len,\n",
    "                        add_start_index=True,\n",
    "                    )\n",
    "    documents = text_splitter.split_documents(raw_docs)\n",
    "    if documents is None or len(documents) == 0:\n",
    "        return \"无法读取文档内容\"\n",
    "    db = Chroma.from_documents(documents, OpenAIEmbeddings(model=\"text-embedding-ada-002\"))\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=OpenAI(\n",
    "            temperature=0,\n",
    "            model_kwargs={\n",
    "                \"seed\": 42\n",
    "            },\n",
    "        ),  # 语言模型\n",
    "        chain_type=\"stuff\",  # prompt的组织方式\n",
    "        retriever=db.as_retriever()  # 检索器\n",
    "    )\n",
    "    response = qa_chain.invoke(query+\"(请用中文回答)\")\n",
    "    return response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-chinese-gSQlHcwW-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
